%=========================================================================
% LaTeX Document
%=========================================================================

\documentclass{cbxposter}

% Automatix LaTeX build system modules

\usepackage{svg}
\usepackage{listings}

% Beamer config

\setbeamertemplate{caption}[numbered]

%-------------------------------------------------------------------------
% Setup
%-------------------------------------------------------------------------

\title
{%
  Using Intra-Core Loop-Task Accelerators \\
  to Improve the Productivity and Performance
  of Task-Based Parallel Programs
}

\authors
{%
  Ji Kim, Shunning Jiang, Christopher Torng \\ [0.15in]
  Moyang Wang, Shreesha Srinath, Berkin Ilbeyi \\ [0.15in]
  Khalid Al-Hawaj, Christopher Batten
}

\inst
{%
  Computer Systems Laboratory \\
  School of Electrical and Computer Engineering,
  Cornell University
}

\name    {Shunning Jiang}
\address {471 Rhodes Hall, Ithaca, NY 14853}
\web     {http://www.csl.cornell.edu}
\email   {sj634@cornell.edu}

%-------------------------------------------------------------------------
% macros
%-------------------------------------------------------------------------

\renewcommand{\medskip}{\vspace{0.5in}}
\renewcommand{\smallskip}{\vspace{0.16667in}}

%-------------------------------------------------------------------------
% Begin Document
%-------------------------------------------------------------------------

\begin{document}
\begin{frame}[fragile,t]{}
\vspace{0.57in}
\begin{columns}[T]

%=========================================================================
% Column 1
%=========================================================================

\begin{column}{10.5in}

%-------------------------------------------------------------------------
% Abstract
%-------------------------------------------------------------------------

\begin{block}{Abstract}

  Task-based parallel programming frameworks offer compelling productivity
  and performance benefits for modern chip multi-processors (CMPs). At the
  same time, CMPs also provide packed-SIMD units to exploit fine-grain
  data parallelism. \BF{Two fundamental challenges make using packed-SIMD
  units with task-parallel programs particularly difficult: (1) the
  intra-core parallel abstraction gap; and (2) inefficient execution of
  irregular tasks.} To address these challenges, we propose \BF{augmenting
  CMPs with intra-core loop-task accelerators (LTAs)}. We introduce a
  lightweight hint in the instruction set to elegantly encode loop-task
  execution and an LTA microarchitectural template that can be configured
  at design time for different amounts of spatial/temporal decoupling to
  efficiently execute both regular and irregular loop tasks. \BF{Compared to
  an in-order CMP baseline, CMP+LTA results in an average speedup of
  4.2\X{} (1.8\X{} area normalized) and similar energy efficiency.
  Compared to an out-of-order CMP baseline, CMP+LTA results in an average
  speedup of 2.3\X{} (1.5\X{} area normalized) and also improves energy
  efficiency by 3.2\X{}}. Our work suggests augmenting CMPs with
  lightweight LTAs can improve performance and efficiency on both regular
  and irregular loop-task parallel programs with minimal software changes.

\end{block}

%-------------------------------------------------------------------------
% Motivation
%-------------------------------------------------------------------------

\vspace{0.81in}
\begin{block}{Motivation}

  Loop-task parallelism is a common parallel pattern usually captured with
  the “parallel for” primitive, where a loop task functor is applied to a
  blocked range. There are two fundamental challenges that make using
  packed-SIMD units in this loop-task context particularly difficult.

  % Task-based frameworks use a software runtime to dynamically map many
  % tasks to fewer threads. Programming with high-level tasks, as opposed to
  % directly using low-level threads, offers many productivity and
  % performance benefits including: an elegant encoding of fine-grain
  % parallelism, implicit synchronization of serial and parallel regions,
  % efficient load-balancing of tasks across threads, and portable
  % performance across a wide range of CMPs.

  \vspace{0.5in}
  \begin{center}
  \includegraphics[width=0.85\tw]{fig-poster-task-to-core.svg.pdf}
  \end{center}
  \vspace{0.5in}

  \begin{cbxlist}{2em}{1.5em}{0em}
    \raggedright

    \item \BF{Intra-Core Parallel Abstraction Gap} -- Two fundamentally
       different parallel abstractions reduce productivity: tasks for
       inter-core parallelism (e.g., TBB) and packed-SIMD for intra-core
       parallelism (e.g., AVX). Auto-vectorization and explicit
       vectorization are challenging to perform since tasks can be
       arbitrarily complex and sizes/alignments are not known at compile
       time, potentially preventing "multiplicative speedup".

    \vspace{0.3in}

    \item \BF{Inefficient Execution of Irregular Tasks} -- Loop tasks are
       often complex with nested loops and function calls, data-dependent
       control flow, indirect memory accesses, and atomic operations
       compared to the scalar implementation. Converting branches into
       arithmetic results in wasted work, extra memory alignment and/or
       data transformations adds overhead, scatter/gather accesses often
       have much lower throughput, and a less efficient algorithm may be
       required for vectorization. All of these reasons derive from the
       fact that the microarchitecture for packed-SIMD extensions is
       fundamentally designed to excel at executing regular data
       parallelism as opposed to the more general loop-task parallelism.
  \end{cbxlist}

  \vspace{0.3in}
  \includegraphics[width=0.93\tw]{fig-poster-motivation-native.py.pdf}
  \vspace{0.3in}

  In this paper, we propose intra-core loop-task accelerators (LTAs) to
  address these challenges. A standard runtime schedules tasks across
  general purpose processors (GPPs) and small software changes enable a
  GPP to use an LTA to accelerate loop-task execution.

  \smallskip
\end{block}
\end{column}

%=========================================================================
% Column 2
%=========================================================================

\begin{column}{10.5in}

%-------------------------------------------------------------------------
% LTA SW
%-------------------------------------------------------------------------

\begin{block}{LTA Software}

  \vspace{-0.1in}
  \cbxsection{Programming Interface}
  \vspace{-0.2in}

  \lstset{language=C++,
          basicstyle=\ttfamily,
          keywordstyle=\color{blue}\ttfamily,
          stringstyle=\color{green}\ttfamily,
          commentstyle=\color{red}\ttfamily,
          morecomment=[l][\color{magenta}]{\#}
  }
  \begin{lstlisting}
  // Element-Wise Vector-Vector Addition
  // with LTA_PARALLEL_FOR Macro

  void vvadd( int dest[], int src0[], int src1[], int size )
  {
    LTA_PARALLEL_FOR( 0, size, (dest,src0,src1), ({
      dest[i] = src0[i] + src1[i];
    }));
  }

  \end{lstlisting}
  \vspace{0.2in}

  The LTA\_PARALLEL\_FOR macro generates an indirect function call
  in-place with runtime management code around it. The destination of the
  indirect function call is the following loop task function.

  \vspace{0.2in}
  \begin{lstlisting}
  // The Loop-Task Function Generated by the Macro

  void task_func( void* a, int start, int end, int step=1 )
  {
    args_t* args = static_cast<args_t*>(a);
    int* dest = args->dest;
    int* src0 = args->src0; int* src1 = args->src1;
    for ( int i = start; i < end; i += step )
      dest[i] = src0[i] + src1[i];
  }
  \end{lstlisting}
  \vspace{0.2in}

  In general, a loop task is a four-tuple of a function pointer, an
  argument pointer, and the start/end indices of the range.

  \vspace{0.1in}
  \cbxsection{ISA Extension: jalr.lta}

  \vspace{-0.3in}
  \begin{center}
  \includegraphics[width=0.7\cw]{fig-poster-jalr-lta.svg.pdf}
  \end{center}

  \vspace{0.2in}
  We propose a new \BF{jalr.lta} instruction that has the same semantics
  as normal indirect function call jalr, but serves as a hint to the
  underlying hardware that the function has the special signature of loop
  task.

%-------------------------------------------------------------------------
% LTA Runtime
%-------------------------------------------------------------------------

  \vspace{0.1in}
  \cbxsection{LTA-Enabled Work-Stealing Runtime}

  The \BF{LTA-enabled work-stealing runtime} still recursively
  partitions loop tasks into subtasks to facilitate load balancing until
  the range is less than the core task size, but then uses the jalr.lta
  instruction. If an LTA is available, the GPP can potentially use the LTA
  to further partition the core task into $\mu$tasks, each responsible
  for a smaller range of iterations. The LTA groups $\mu$tasks into task
  groups which execute on a set of $\mu$threads in lockstep (i.e., same
  instruction), exploiting structure for efficient execution.

  \vspace{0.5in}
  \includegraphics[width=0.93\cw]{fig-poster-lta-runtime-v4.svg.pdf}
  \vspace{0.5in}

  If an LTA is not available (GPP 2 in the above diagram), a jalr.lta can
  be treated as a standard jalr executing on the GPP. This approach
  requires minimal changes to a standard work-stealing runtime and
  practically no changes to the parallel program. Compare this to the
  significant software changes required to combine task-parallel
  programming and packed-SIMD extensions.

    \smallskip

\end{block}
\end{column}

%=========================================================================
% Column 3
%=========================================================================

\begin{column}{10.5in}

%-------------------------------------------------------------------------
% LTA Microarchitecture
%-------------------------------------------------------------------------

\begin{block}{LTA Hardware}
  \vspace{-0.1in}
  \cbxsection{Spatial and Temporal Task Coupling}
  \includegraphics[width=0.93\cw]{fig-poster-lta-execution-tc-lc.svg.pdf}

  \cbxsection{Task Coupling Taxonomy}
  \includegraphics[width=0.9\cw]{fig-poster-lta-taxonomy.svg.pdf}

  \cbxsection{Microarchitecture Template}
  \includegraphics[width=0.93\cw]{fig-poster-lta-uarch.svg.pdf}

\end{block}

%-------------------------------------------------------------------------
% Acknowledgments
%-------------------------------------------------------------------------

% \vspace{0.81in}
% \begin{block}{Acknowledgements}
%\cbxsection{Acknowledgments}

  \vspace{0.25in}

  This work was supported in part by NSF CAREER Award \#1149464, NSF XPS
  Award \#1337240, NSF CRI Award \#1512937, NSF SHF Award \#1527065,
  AFOSR YIP Award \#FA9550-15-1-0194, and donations from Intel, NVIDIA,
  and Synopsys. The authors acknowledge and thank Scott McKenzie and
  Alvin Wijaya for their early work on LTA RTL modeling, Jason Setter and
  Wei Geng for their work on scalar processor PyMTL modeling, and David
  Bindel for access to an Intel Xeon Phi 5110P.

  % \end{block}

\end{column}

%=========================================================================
% Column 4
%=========================================================================

\begin{column}{10.5in}

%-------------------------------------------------------------------------
% Cycle-Level Evaluation
%-------------------------------------------------------------------------

\begin{block}{Cycle-Level Evaluation}

  We utilize a co-simulation framework with gem5 and PyMTL, a Python-based
  hardware modeling framework. The cycle-level models for LTAs were
  implemented in PyMTL. Each core and its LTA share the L1 caches and all
  cores share the L2 cache. We simulate a bare-metal system with system
  call emulation. We ported 16 C++ application kernels to a MIPS-like
  architecture. We used a cross-compiler based on GCC-4.4.1,
  Newlib-1.17.0, and the GNU standard C++ library. Application kernels
  were either ported from Problem Based Benchmark Suite (PBBS) or
  developed in-house to create a suite with diverse task-level and
  instruction-level characteristics.

  \cbxsection{Performance vs. HW Resource}

  \begin{center}

  \vspace{-0.2in}
  \includegraphics[width=0.93\cw]{fig-poster-eval-reduce-resource0.py.pdf}
  \vspace{-0.025in}

  128-$\mu$thread LTA with aggressive front-end, eight memory ports, and
  eight LLFUs.

  \vspace{0.3in}
  \includegraphics[width=0.93\cw]{fig-poster-eval-reduce-resource1.py.pdf}
  \vspace{-0.025in}

  128-$\mu$thread LTA with \BF{realistic front-end}, eight memory ports,
  and eight LLFUs.

  \vspace{0.3in}
  \includegraphics[width=0.93\cw]{fig-poster-eval-reduce-resource2.py.pdf}
  \vspace{-0.025in}

  \BF{32-$\mu$thread LTA} with realistic front-end, eight memory ports,
  and eight LLFUs.

  \vspace{0.3in}
  \includegraphics[width=0.93\cw]{fig-poster-eval-reduce-resource3.py.pdf}
  \vspace{-0.025in}

  32-$\mu$thread LTA with realistic front-end, \BF{two memory ports}, and
  eight LLFUs.

  \vspace{0.3in}
  \includegraphics[width=0.93\cw]{fig-poster-eval-reduce-resource4.py.pdf}
  \vspace{-0.025in}

  32-$\mu$thread LTA with realistic front-end, two memory ports, and
  \BF{four LLFUs}.
  \end{center}

  \cbxsection{Chip-Multiprocessor(CMP) with LTA}

  \includegraphics[width=0.93\cw]{fig-poster-eval-perf-mt.py.pdf}

  Compared to CMP-IO, CMP+LTA improves average raw performance by up to
  4.2\X{}, performance per area by 1.8\X{} (excluding the L2), and energy
  efficiency by 1.1\X{}.

  Compared to a more aggressive CMP-O3, CMP+LTA improves
  performance by 2.3\X{}, performance per area by 1.5\X{}
  (excluding the L2), and energy efficiency by 3.2\X{}.

  \vspace{0.1in}

  Using the jalr.lta instruction closes the intra-core parallel
  abstraction gap, and allows porting the kernels from TBB
  implementations with minimal changes. A single implementation is
  written and compiled once, then executed on a system with any
  combination of GPPs and homogeneous or heterogeneous LTAs.

  \smallskip
\end{block}

%-------------------------------------------------------------------------
% End Document
%-------------------------------------------------------------------------

\end{column}
\end{columns}
\end{frame}
\end{document}
